# [Apache Spark](http://spark.apache.org/)

Spark is a fast and general cluster computing system for Big Data. It provides
high-level APIs in Scala, Java, Python, and R, and an optimized engine that
supports general computation graphs for data analysis.


- [Wiki](https://cwiki.apache.org/confluence/display/SPARK)
- [Building](http://spark.apache.org/docs/latest/building-spark.html)
- [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)

## Pre-requisites

Four using **Jupiter Notebooks** throw Visual Studio code

1. Install VS Code
2. Install Anaconda/Miniconda or Python environment with Jupyter package installed
3. Install the Jupyter Extension and the Python Extension
4. Open or create a notebook file by opening the Command Palette (Ctrl+Shift+P) and select Jupyter: Create New Jupyter Notebook.

# Docker image with spark-py setup
1. Install docker
2. Pull spark-py image from [docker Hub](https://hub.docker.com/r/apache/spark-py/tags) \
`docker pull apache/spark-py:latest`